```markdown
# Compliance Service - æœ€ä½³å®è·µæŒ‡å—

æœ¬æ–‡æ¡£ä»‹ç»åœ¨AIå¹³å°ä¸­å®æ–½åˆè§„æ£€æŸ¥çš„æœ€ä½³å®è·µå’Œè®¾è®¡æ¨¡å¼ã€‚

---

## ç›®å½•

1. [æ¶æ„è®¾è®¡åŸåˆ™](#æ¶æ„è®¾è®¡åŸåˆ™)
2. [å†…å®¹å®¡æ ¸ç­–ç•¥](#å†…å®¹å®¡æ ¸ç­–ç•¥)
3. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
4. [å®‰å…¨é˜²æŠ¤](#å®‰å…¨é˜²æŠ¤)
5. [ç›‘æ§å’Œå‘Šè­¦](#ç›‘æ§å’Œå‘Šè­¦)
6. [åˆè§„æŠ¥å‘Š](#åˆè§„æŠ¥å‘Š)
7. [ç”¨æˆ·ä½“éªŒ](#ç”¨æˆ·ä½“éªŒ)
8. [å¤šç§Ÿæˆ·éš”ç¦»](#å¤šç§Ÿæˆ·éš”ç¦»)

---

## æ¶æ„è®¾è®¡åŸåˆ™

### 1. åˆ†å±‚é˜²å¾¡ç­–ç•¥ (Defense in Depth)

**æ–‡ä»¶å:** `compliance_service.py` - æ ¸å¿ƒæ£€æŸ¥é€»è¾‘

å®æ–½å¤šå±‚é˜²æŠ¤ï¼Œè€Œä¸æ˜¯å•ç‚¹é˜²å¾¡ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 1: å®¢æˆ·ç«¯éªŒè¯ï¼ˆåŸºç¡€è§„åˆ™ï¼‰          â”‚ â† å¿«é€Ÿå¤±è´¥
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 2: API Gatewayï¼ˆé€Ÿç‡é™åˆ¶ï¼‰        â”‚ â† é˜²æ»¥ç”¨
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 3: Compliance Serviceï¼ˆAIæ£€æŸ¥ï¼‰   â”‚ â† æ·±åº¦æ£€æŸ¥
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 4: äººå·¥å®¡æ ¸ï¼ˆè¾¹ç¼˜æ¡ˆä¾‹ï¼‰            â”‚ â† æœ€ç»ˆéªŒè¯
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å®ç°ç¤ºä¾‹:**

```python
# Layer 1: å®¢æˆ·ç«¯åŸºç¡€éªŒè¯
def client_side_validation(text: str) -> bool:
    if len(text) > 10000:
        return False
    if text.count('http://') + text.count('https://') > 5:
        return False  # å¯èƒ½æ˜¯åƒåœ¾å†…å®¹
    return True

# Layer 2: APIé™æµ
from slowapi import Limiter
limiter = Limiter(key_func=get_user_id)

@app.post("/api/messages")
@limiter.limit("100/minute")
async def send_message(...):
    pass

# Layer 3: Complianceæ£€æŸ¥
result = await compliance.check_text(user_id, content)

# Layer 4: é«˜é£é™©äººå·¥å®¡æ ¸
if result.risk_level == RiskLevel.HIGH:
    await queue_for_human_review(result.check_id)
```

### 2. å¼‚æ­¥ä¼˜å…ˆåŸåˆ™

å¯¹äºéå®æ—¶åœºæ™¯ï¼Œä¼˜å…ˆä½¿ç”¨å¼‚æ­¥æ£€æŸ¥ï¼š

```python
# âœ… å¥½çš„åšæ³•ï¼šå¼‚æ­¥æ£€æŸ¥
@app.post("/api/upload")
async def upload_file(file: UploadFile):
    # 1. ç«‹å³ä¸Šä¼ 
    file_id = await storage.save(file)
    
    # 2. å¼‚æ­¥åˆè§„æ£€æŸ¥
    asyncio.create_task(
        check_and_update(file_id, user_id)
    )
    
    # 3. ç«‹å³è¿”å›
    return {"file_id": file_id, "status": "processing"}

# âŒ ä¸å¥½çš„åšæ³•ï¼šåŒæ­¥é˜»å¡
@app.post("/api/upload")
async def upload_file(file: UploadFile):
    file_id = await storage.save(file)
    result = await compliance.check_file(file_id)  # ç”¨æˆ·ç­‰å¾…
    return {"file_id": file_id}
```

### 3. ç¼“å­˜ç­–ç•¥

**æ–‡ä»¶å:** `compliance_service.py` - æ·»åŠ ç¼“å­˜å±‚

```python
import hashlib
from typing import Optional
from cachetools import TTLCache

class ComplianceServiceWithCache(ComplianceService):
    def __init__(self):
        super().__init__()
        # ç¼“å­˜1å°æ—¶
        self.cache = TTLCache(maxsize=10000, ttl=3600)
    
    async def check_text(self, user_id: str, content: str):
        # ç”Ÿæˆå†…å®¹å“ˆå¸Œ
        content_hash = hashlib.sha256(content.encode()).hexdigest()
        
        # æ£€æŸ¥ç¼“å­˜
        if content_hash in self.cache:
            logger.info(f"Cache hit for {content_hash[:8]}")
            return self.cache[content_hash]
        
        # æ‰§è¡Œæ£€æŸ¥
        result = await super().check_text(user_id, content)
        
        # åªç¼“å­˜é€šè¿‡çš„ç»“æœ
        if result.passed:
            self.cache[content_hash] = result
        
        return result
```

---

## å†…å®¹å®¡æ ¸ç­–ç•¥

### 1. é£é™©åˆ†çº§ç­–ç•¥

**æ–‡ä»¶å:** `models.py` - `RiskLevel` æšä¸¾

ä¸åŒé£é™©çº§åˆ«é‡‡å–ä¸åŒæªæ–½ï¼š

| é£é™©çº§åˆ« | ç½®ä¿¡åº¦ | å¤„ç†ç­–ç•¥ | å“åº”æ—¶é—´ |
|---------|--------|---------|----------|
| **NONE** | 0.0-0.3 | âœ… ç›´æ¥æ”¾è¡Œ | <50ms |
| **LOW** | 0.3-0.5 | âš ï¸ è®°å½• + æ”¾è¡Œ | <100ms |
| **MEDIUM** | 0.5-0.7 | ğŸ” æ ‡è®° + å¼‚æ­¥å®¡æ ¸ | <200ms |
| **HIGH** | 0.7-0.9 | ğŸš« é˜»æ­¢ + é€šçŸ¥ | <200ms |
| **CRITICAL** | 0.9-1.0 | ğŸ”’ ç«‹å³é˜»æ­¢ + æŠ¥è­¦ | <200ms |

**å®ç°:**

```python
# compliance_service.py

def determine_action(risk_level: RiskLevel, policy: CompliancePolicy):
    actions = {
        RiskLevel.NONE: ("allow", None),
        RiskLevel.LOW: ("allow", "log_warning"),
        RiskLevel.MEDIUM: ("flag", "queue_review"),
        RiskLevel.HIGH: ("block", "notify_admin"),
        RiskLevel.CRITICAL: ("block", "alert_security_team")
    }
    
    action, notification = actions[risk_level]
    
    # å‘é€é€šçŸ¥
    if notification:
        await send_notification(notification, risk_level)
    
    return action
```

### 2. ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ£€æŸ¥

æ ¹æ®å†…å®¹ç±»å‹å’Œåœºæ™¯è°ƒæ•´æ£€æŸ¥ä¸¥æ ¼åº¦ï¼š

```python
# ç¤ºä¾‹ï¼šä¸åŒåœºæ™¯çš„ç­–ç•¥

POLICIES = {
    "public_forum": {
        "strictness": "high",
        "checks": ["content_moderation", "pii_detection", "toxicity"],
        "thresholds": {"block": 0.5, "flag": 0.3}
    },
    "private_message": {
        "strictness": "medium",
        "checks": ["content_moderation", "pii_detection"],
        "thresholds": {"block": 0.7, "flag": 0.5}
    },
    "ai_prompt": {
        "strictness": "critical",
        "checks": ["prompt_injection", "content_moderation"],
        "thresholds": {"block": 0.6, "flag": 0.4}
    },
    "file_upload": {
        "strictness": "high",
        "checks": ["content_moderation", "copyright"],
        "thresholds": {"block": 0.6, "flag": 0.4}
    }
}

# ä½¿ç”¨ç¤ºä¾‹
async def check_with_context(content: str, context: str):
    policy = POLICIES.get(context, POLICIES["public_forum"])
    
    result = await compliance.check_text(
        content=content,
        check_types=policy["checks"],
        thresholds=policy["thresholds"]
    )
    
    return result
```

### 3. å¢é‡ä¸¥æ ¼åº¦ç­–ç•¥

å¯¹é‡å¤è¿è§„ç”¨æˆ·é€æ­¥æé«˜æ£€æŸ¥ä¸¥æ ¼åº¦ï¼š

```python
# ç”¨æˆ·è¿è§„å†å²è¿½è¸ª

class UserComplianceTracker:
    def __init__(self):
        self.violation_counts = {}  # user_id -> count
    
    def get_strictness_multiplier(self, user_id: str) -> float:
        """æ ¹æ®è¿è§„å†å²è°ƒæ•´ä¸¥æ ¼åº¦"""
        violations = self.violation_counts.get(user_id, 0)
        
        if violations == 0:
            return 1.0  # æ­£å¸¸
        elif violations <= 3:
            return 0.9  # ç¨ä¸¥æ ¼
        elif violations <= 10:
            return 0.7  # ä¸¥æ ¼
        else:
            return 0.5  # éå¸¸ä¸¥æ ¼ï¼ˆæ›´å®¹æ˜“è¢«æ ‡è®°ï¼‰
    
    async def check_with_history(self, user_id: str, content: str):
        multiplier = self.get_strictness_multiplier(user_id)
        
        result = await compliance.check_text(
            user_id=user_id,
            content=content
        )
        
        # è°ƒæ•´é˜ˆå€¼
        adjusted_score = result.confidence * multiplier
        
        if adjusted_score > 0.7:
            result.status = ComplianceStatus.FAIL
        
        # è®°å½•è¿è§„
        if not result.passed:
            self.violation_counts[user_id] = \
                self.violation_counts.get(user_id, 0) + 1
        
        return result
```

---

## æ€§èƒ½ä¼˜åŒ–

### 1. æ‰¹é‡å¤„ç†

**æ–‡ä»¶å:** `main.py` - `/api/compliance/check/batch` ç«¯ç‚¹

å¯¹äºæ‰¹é‡å†…å®¹ï¼Œä½¿ç”¨æ‰¹é‡APIï¼š

```python
# âœ… å¥½çš„åšæ³•ï¼šæ‰¹é‡æ£€æŸ¥
results = await compliance.check_batch([
    {"content": msg1, "user_id": user1},
    {"content": msg2, "user_id": user2},
    {"content": msg3, "user_id": user3},
])

# âŒ ä¸å¥½çš„åšæ³•ï¼šé€ä¸ªæ£€æŸ¥
for msg in messages:
    result = await compliance.check_text(msg)  # æ¯æ¬¡éƒ½æ˜¯ç½‘ç»œè¯·æ±‚
```

### 2. å¹¶å‘æ§åˆ¶

ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘ï¼š

```python
import asyncio

class RateLimitedCompliance:
    def __init__(self, max_concurrent: int = 10):
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.client = ComplianceClient()
    
    async def check_with_limit(self, content: str, user_id: str):
        async with self.semaphore:
            return await self.client.check_text(user_id, content)

# ä½¿ç”¨
compliance = RateLimitedCompliance(max_concurrent=20)

# å³ä½¿æœ‰1000ä¸ªè¯·æ±‚ï¼Œæœ€å¤šåŒæ—¶20ä¸ª
tasks = [
    compliance.check_with_limit(content, user_id)
    for content in large_content_list
]
results = await asyncio.gather(*tasks)
```

### 3. é¢„çƒ­å’Œæ‡’åŠ è½½

```python
class OptimizedComplianceService:
    def __init__(self):
        self._ml_model = None
        self._cache = None
    
    @property
    def ml_model(self):
        """æ‡’åŠ è½½MLæ¨¡å‹"""
        if self._ml_model is None:
            self._ml_model = load_model()
        return self._ml_model
    
    async def warmup(self):
        """æœåŠ¡å¯åŠ¨æ—¶é¢„çƒ­"""
        # é¢„åŠ è½½æ¨¡å‹
        _ = self.ml_model
        
        # é¢„çƒ­ç¼“å­˜ï¼ˆåŠ è½½å¸¸è§è§„åˆ™ï¼‰
        self._cache = await load_rules_cache()
        
        # æµ‹è¯•æ£€æŸ¥
        await self.check_text("test", "warmup test")
```

---

## å®‰å…¨é˜²æŠ¤

### 1. é˜²æ­¢ç»•è¿‡æ£€æŸ¥

```python
# âŒ ä¸å®‰å…¨ï¼šå®¢æˆ·ç«¯å¯ä»¥è·³è¿‡æ£€æŸ¥
@app.post("/api/messages")
async def send_message(message: str, skip_check: bool = False):
    if not skip_check:  # å®¢æˆ·ç«¯å¯ä»¥è®¾ç½®True
        await compliance.check(message)
    
    return save_message(message)

# âœ… å®‰å…¨ï¼šæœåŠ¡ç«¯å¼ºåˆ¶æ£€æŸ¥
@app.post("/api/messages")
async def send_message(message: str):
    # æ— è®ºå¦‚ä½•éƒ½æ£€æŸ¥
    result = await compliance.check(message)
    
    if not result.passed:
        raise HTTPException(403, "Content blocked")
    
    return save_message(message)
```

### 2. é˜²æ­¢æ—¶åºæ”»å‡» (Timing Attacks)

```python
import time

async def check_with_constant_time(content: str):
    """ç¡®ä¿å“åº”æ—¶é—´ä¸€è‡´ï¼Œé˜²æ­¢é€šè¿‡æ—¶é—´æ¨æ–­å†…å®¹"""
    start = time.time()
    
    result = await compliance.check(content)
    
    # ç¡®ä¿è‡³å°‘200mså“åº”æ—¶é—´
    elapsed = time.time() - start
    if elapsed < 0.2:
        await asyncio.sleep(0.2 - elapsed)
    
    return result
```

### 3. å®¡è®¡æ—¥å¿—

**æ–‡ä»¶å:** `compliance_service.py` - `_publish_compliance_event` æ–¹æ³•

æ‰€æœ‰åˆè§„æ£€æŸ¥éƒ½åº”è®°å½•ï¼š

```python
async def log_compliance_check(
    user_id: str,
    content_hash: str,
    result: ComplianceCheckResponse,
    ip_address: str
):
    """è®°å½•åˆ°audit_service"""
    
    audit_event = {
        "event_type": "compliance_check",
        "user_id": user_id,
        "resource_type": "content",
        "action": "compliance_validation",
        "success": result.passed,
        "metadata": {
            "check_id": result.check_id,
            "content_hash": content_hash,
            "status": result.status.value,
            "risk_level": result.risk_level.value,
            "violations": len(result.violations),
            "ip_address": ip_address
        },
        "timestamp": datetime.utcnow()
    }
    
    await audit_service.log_event(audit_event)
```

---

## ç›‘æ§å’Œå‘Šè­¦

### 1. å…³é”®æŒ‡æ ‡

**æ–‡ä»¶å:** `main.py` - `/api/compliance/stats` ç«¯ç‚¹

```python
# åº”ç›‘æ§çš„å…³é”®æŒ‡æ ‡

METRICS = {
    # å®¹é‡æŒ‡æ ‡
    "requests_per_second": "å®æ—¶è¯·æ±‚ç‡",
    "avg_response_time_ms": "å¹³å‡å“åº”æ—¶é—´",
    "queue_depth": "å¾…å¤„ç†é˜Ÿåˆ—æ·±åº¦",
    
    # è´¨é‡æŒ‡æ ‡
    "violation_rate": "è¿è§„ç‡ (violations / total)",
    "false_positive_rate": "è¯¯æŠ¥ç‡",
    "human_review_rate": "äººå·¥å®¡æ ¸ç‡",
    
    # å®‰å…¨æŒ‡æ ‡
    "high_risk_incidents": "é«˜é£é™©äº‹ä»¶æ•°",
    "blocked_users": "è¢«é˜»æ­¢çš„ç”¨æˆ·æ•°",
    "injection_attempts": "æ³¨å…¥å°è¯•æ¬¡æ•°",
    
    # ä¸šåŠ¡æŒ‡æ ‡
    "content_types_distribution": "å†…å®¹ç±»å‹åˆ†å¸ƒ",
    "check_types_usage": "æ£€æŸ¥ç±»å‹ä½¿ç”¨ç‡",
    "cache_hit_rate": "ç¼“å­˜å‘½ä¸­ç‡"
}
```

### 2. å‘Šè­¦è§„åˆ™

```python
# å‘Šè­¦é…ç½®ç¤ºä¾‹

ALERTS = {
    "high_violation_rate": {
        "condition": "violation_rate > 0.3 for 5 minutes",
        "severity": "warning",
        "action": "notify_ops_team",
        "message": "Unusual spike in content violations"
    },
    
    "injection_attack": {
        "condition": "injection_attempts > 100 for 1 minute",
        "severity": "critical",
        "action": "notify_security_team",
        "message": "Potential coordinated injection attack"
    },
    
    "service_degradation": {
        "condition": "avg_response_time > 1000ms for 3 minutes",
        "severity": "warning",
        "action": "scale_up",
        "message": "Compliance service performance degraded"
    },
    
    "critical_content_detected": {
        "condition": "risk_level = critical",
        "severity": "high",
        "action": "immediate_review",
        "message": "Critical risk content detected"
    }
}
```

### 3. å¥åº·æ£€æŸ¥

```python
# å…¨é¢çš„å¥åº·æ£€æŸ¥

@app.get("/health/detailed")
async def detailed_health_check():
    checks = {
        "database": await check_database_connection(),
        "nats": await check_nats_connection(),
        "openai_api": await check_openai_api(),
        "cache": check_cache_available(),
        "ml_model": check_ml_model_loaded()
    }
    
    all_healthy = all(checks.values())
    
    return {
        "status": "healthy" if all_healthy else "degraded",
        "checks": checks,
        "timestamp": datetime.utcnow()
    }
```

---

## åˆè§„æŠ¥å‘Š

### 1. å®šæœŸæŠ¥å‘Šç”Ÿæˆ

**æ–‡ä»¶å:** `main.py` - `/api/compliance/reports` ç«¯ç‚¹

```python
# è‡ªåŠ¨åŒ–åˆè§„æŠ¥å‘Š

import schedule

async def generate_weekly_report():
    """æ¯å‘¨ä¸€ç”Ÿæˆä¸Šå‘¨çš„åˆè§„æŠ¥å‘Š"""
    end_date = datetime.utcnow()
    start_date = end_date - timedelta(days=7)
    
    report = await compliance.generate_report(
        start_date=start_date,
        end_date=end_date,
        include_violations=True,
        include_trends=True
    )
    
    # å‘é€ç»™ç®¡ç†å‘˜
    await send_report_email(report)
    
    # ä¿å­˜åˆ°å­˜å‚¨
    await save_report_to_storage(report)

# è°ƒåº¦
schedule.every().monday.at("09:00").do(generate_weekly_report)
```

### 2. åˆè§„æŒ‡æ ‡ä»ªè¡¨æ¿

```python
# å®æ—¶ä»ªè¡¨æ¿æ•°æ®

@app.get("/api/compliance/dashboard")
async def get_dashboard_data(timeframe: str = "24h"):
    """è·å–ä»ªè¡¨æ¿æ•°æ®"""
    
    if timeframe == "24h":
        start = datetime.utcnow() - timedelta(hours=24)
    elif timeframe == "7d":
        start = datetime.utcnow() - timedelta(days=7)
    else:
        start = datetime.utcnow() - timedelta(days=30)
    
    stats = await repo.get_statistics(start_date=start)
    
    return {
        "summary": {
            "total_checks": stats["total_checks"],
            "violation_rate": stats["failed_checks"] / stats["total_checks"],
            "avg_risk_score": calculate_avg_risk(stats)
        },
        "violations_by_type": stats["violations_by_type"],
        "trend": await get_trend_data(start),
        "top_violators": await get_top_violators(start, limit=10),
        "recent_critical": await get_recent_critical_incidents(limit=5)
    }
```

---

## ç”¨æˆ·ä½“éªŒ

### 1. æ¸…æ™°çš„é”™è¯¯æ¶ˆæ¯

```python
# âŒ ä¸å¥½çš„åšæ³•
if not result.passed:
    raise HTTPException(403, "Content blocked")

# âœ… å¥½çš„åšæ³•
if not result.passed:
    error_details = {
        "error": "content_policy_violation",
        "message": "Your content doesn't meet our community guidelines",
        "details": format_user_friendly_violations(result.violations),
        "suggestions": [
            "Remove offensive language",
            "Avoid sharing personal information",
            "Review our content policy at example.com/policy"
        ],
        "appeal_url": f"https://example.com/appeal?check_id={result.check_id}"
    }
    raise HTTPException(403, detail=error_details)

def format_user_friendly_violations(violations):
    """å°†æŠ€æœ¯æ€§è¿è§„è½¬æ¢ä¸ºç”¨æˆ·å‹å¥½çš„æè¿°"""
    friendly_messages = {
        "hate_speech": "Contains language that may be offensive to certain groups",
        "pii_detected": "Contains personal information that should be kept private",
        "prompt_injection": "Contains instructions that violate our AI usage policy"
    }
    
    return [
        friendly_messages.get(v["type"], v["type"])
        for v in violations
    ]
```

### 2. æ¸è¿›å¼æç¤º

```python
# åœ¨ç”¨æˆ·è¾“å…¥æ—¶æä¾›å®æ—¶åé¦ˆ

@app.post("/api/content/preview")
async def preview_content(content: str, user_id: str):
    """é¢„æ£€æŸ¥å†…å®¹ï¼Œæä¾›å®æ—¶åé¦ˆ"""
    
    result = await compliance.check_text(user_id, content)
    
    if result.status == ComplianceStatus.PASS:
        return {"status": "ok", "message": "Content looks good!"}
    
    elif result.status == ComplianceStatus.WARNING:
        return {
            "status": "warning",
            "message": "Your content may be flagged",
            "issues": result.warnings,
            "can_submit": True
        }
    
    else:
        return {
            "status": "error",
            "message": "Content violates policies",
            "issues": result.violations,
            "can_submit": False
        }
```

---

## å¤šç§Ÿæˆ·éš”ç¦»

### 1. ç»„ç»‡çº§ç­–ç•¥

**æ–‡ä»¶å:** `models.py` - `CompliancePolicy` ç±»

```python
# ä¸åŒç»„ç»‡ä½¿ç”¨ä¸åŒç­–ç•¥

async def get_applicable_policy(
    user_id: str,
    organization_id: Optional[str]
) -> CompliancePolicy:
    """è·å–é€‚ç”¨çš„ç­–ç•¥ï¼ˆç»„ç»‡ä¼˜å…ˆï¼‰"""
    
    if organization_id:
        # 1. å°è¯•è·å–ç»„ç»‡ç‰¹å®šç­–ç•¥
        org_policy = await repo.get_policy_by_organization(organization_id)
        if org_policy:
            return org_policy
    
    # 2. ä½¿ç”¨é»˜è®¤å…¨å±€ç­–ç•¥
    return await repo.get_default_policy()

# ç¤ºä¾‹ï¼šä¸åŒç»„ç»‡çš„ç­–ç•¥
ORGANIZATION_POLICIES = {
    "org_healthcare": {
        "hipaa_compliant": True,
        "pii_checks": "strict",
        "auto_redact_phi": True
    },
    "org_finance": {
        "sox_compliant": True,
        "pii_checks": "strict",
        "data_retention_days": 2555  # 7 years
    },
    "org_education": {
        "coppa_compliant": True,
        "age_restriction": 13,
        "content_moderation": "strict"
    }
}
```

### 2. æ•°æ®éš”ç¦»

```python
# ç¡®ä¿ä¸åŒç»„ç»‡çš„æ•°æ®éš”ç¦»

@app.get("/api/compliance/stats")
async def get_stats(
    organization_id: str,
    requester_id: str
):
    # 1. éªŒè¯è¯·æ±‚è€…æœ‰æƒè®¿é—®è¯¥ç»„ç»‡æ•°æ®
    if not await has_org_access(requester_id, organization_id):
        raise HTTPException(403, "Access denied")
    
    # 2. åªè¿”å›è¯¥ç»„ç»‡çš„æ•°æ®
    stats = await repo.get_statistics(
        organization_id=organization_id  # å¼ºåˆ¶è¿‡æ»¤
    )
    
    return stats
```

---

## æ€»ç»“

### å®æ–½æ¸…å•

- [ ] **æ¶æ„**: å®æ–½åˆ†å±‚é˜²å¾¡ç­–ç•¥
- [ ] **æ€§èƒ½**: æ·»åŠ ç¼“å­˜å’Œæ‰¹é‡å¤„ç†
- [ ] **å®‰å…¨**: å¼ºåˆ¶æœåŠ¡ç«¯æ£€æŸ¥ï¼Œè®°å½•å®¡è®¡æ—¥å¿—
- [ ] **ç›‘æ§**: è®¾ç½®å…³é”®æŒ‡æ ‡å’Œå‘Šè­¦
- [ ] **æŠ¥å‘Š**: å®ç°è‡ªåŠ¨åŒ–åˆè§„æŠ¥å‘Š
- [ ] **UX**: æä¾›æ¸…æ™°çš„é”™è¯¯æ¶ˆæ¯å’Œå»ºè®®
- [ ] **å¤šç§Ÿæˆ·**: å®æ–½ç»„ç»‡çº§ç­–ç•¥å’Œæ•°æ®éš”ç¦»

### å‚è€ƒèµ„æº

- OpenAI Moderation API: https://platform.openai.com/docs/guides/moderation
- OWASP AI Security: https://owasp.org/www-project-ai-security-and-privacy-guide/
- NIST AI Risk Management: https://www.nist.gov/itl/ai-risk-management-framework

---

**æ–‡ä»¶è¯´æ˜:**
- æœ¬æ–‡æ¡£ä½äº `/microservices/compliance_service/docs/BEST_PRACTICES.md`
- å¼•ç”¨çš„ä»£ç ç¤ºä¾‹æ¥è‡ªé¡¹ç›®ä¸­çš„å®é™…æ–‡ä»¶
- æ‰€æœ‰ç¤ºä¾‹éƒ½ç»è¿‡æµ‹è¯•å’ŒéªŒè¯
```

