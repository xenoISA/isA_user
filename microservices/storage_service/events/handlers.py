"""
Storage Service - Event Handlers

å¤„ç†æ¥æ”¶åˆ°çš„äº‹ä»¶ï¼ˆè®¢é˜…å…¶ä»–æœåŠ¡äº‹ä»¶ï¼ŒåŒ…æ‹¬è‡ªå·±çš„å¼‚æ­¥äº‹ä»¶ï¼‰
"""

import logging
from datetime import datetime

from core.nats_client import Event, EventType, ServiceSource

logger = logging.getLogger(__name__)


async def handle_file_indexing_request(
    event: Event, intelligence_service, storage_service, event_bus
):
    """
    å¤„ç†æ–‡ä»¶ç´¢å¼•è¯·æ±‚äº‹ä»¶ï¼ˆå¼‚æ­¥åå°ä»»åŠ¡ï¼‰

    å½“æ–‡ä»¶ä¸Šä¼ æ—¶ï¼Œä¼šå‘å¸ƒ FILE_INDEXING_REQUESTED äº‹ä»¶
    æ­¤ handler åœ¨åå°å¼‚æ­¥å¤„ç†ç´¢å¼•å·¥ä½œ
    """
    try:
        logger.info(f"Received file indexing request: {event.id}")

        # æå–äº‹ä»¶æ•°æ®
        data = event.data
        file_id = data.get("file_id")
        user_id = data.get("user_id")
        organization_id = data.get("organization_id")
        file_name = data.get("file_name")
        file_type = data.get("file_type")
        file_size = data.get("file_size")
        metadata = data.get("metadata", {})
        tags = data.get("tags", [])
        bucket_name = data.get("bucket_name")
        object_name = data.get("object_name")

        if not all([file_id, user_id, file_name, bucket_name, object_name]):
            logger.error(f"Missing required fields in indexing request: {data}")
            return

        # å‡†å¤‡æ–‡ä»¶å†…å®¹/URL
        try:
            # å¯¹äºå›¾ç‰‡å’ŒPDFï¼Œä¼ é€’é¢„ç­¾åURL
            if file_type.startswith("image/") or file_type == "application/pdf":
                file_content = storage_service.minio_client.get_presigned_url(
                    bucket_name=bucket_name,
                    object_key=object_name,
                    expiry_seconds=86400,  # 24å°æ—¶ï¼Œç”¨äºAIå¤„ç†
                )
                if not file_content:
                    raise Exception("Failed to generate presigned URL")
                logger.info(
                    f"Generated presigned URL for {file_type}: {file_content[:100]}..."
                )
            else:
                # å¯¹äºæ–‡æœ¬æ–‡ä»¶ï¼Œä¸‹è½½å†…å®¹
                file_bytes = storage_service.minio_client.get_object(
                    bucket_name, object_name
                )
                if file_bytes is None:
                    raise Exception("File not found in MinIO")
                file_content = file_bytes.decode("utf-8", errors="ignore")

        except Exception as e:
            logger.error(f"Failed to download file {file_id} from MinIO: {e}")
            # å‘å¸ƒç´¢å¼•å¤±è´¥äº‹ä»¶
            if event_bus:
                from .publishers import StorageEventPublisher

                publisher = StorageEventPublisher(event_bus)
                await publisher.publish_file_indexing_failed(
                    file_id=file_id,
                    user_id=user_id,
                    error=f"Failed to download file: {str(e)}",
                )
            return

        # é€šè¿‡ intelligence service ç´¢å¼•æ–‡ä»¶
        try:
            logger.info(f"Starting async indexing for file {file_id}")
            # Add bucket_name and object_name to metadata for AI extraction
            if metadata is None:
                metadata = {}
            metadata.update({"bucket_name": bucket_name, "object_name": object_name})

            indexed_doc = await intelligence_service.index_file(
                file_id=file_id,
                user_id=user_id,
                organization_id=organization_id,
                file_name=file_name,
                file_content=file_content,
                file_type=file_type,
                file_size=file_size,
                metadata=metadata,
                tags=tags,
            )

            logger.info(f"Successfully indexed file {file_id}")

            # å‘å¸ƒç´¢å¼•æˆåŠŸäº‹ä»¶
            if event_bus:
                from .publishers import StorageEventPublisher

                publisher = StorageEventPublisher(event_bus)
                await publisher.publish_file_indexed(
                    file_id=file_id,
                    user_id=user_id,
                    file_name=file_name,
                    file_size=file_size,
                )

                # åŒæ—¶å‘å¸ƒ file.uploaded.with_ai äº‹ä»¶ç»™ Media Serviceï¼ˆå‘åå…¼å®¹ï¼‰
                # Media Service æœŸæœ›æ¥æ”¶å¸¦æœ‰ AI metadata çš„äº‹ä»¶
                if file_type and file_type.startswith("image/") and indexed_doc:
                    # ä»ç´¢å¼•ç»“æœçš„ metadata ä¸­æå– AI metadata
                    doc_metadata = indexed_doc.metadata or {}
                    logger.info(
                        f"ğŸ” DEBUG: indexed_doc.metadata keys: {list(doc_metadata.keys())}"
                    )
                    logger.info(f"ğŸ” DEBUG: indexed_doc.metadata: {doc_metadata}")

                    ai_metadata_extracted = doc_metadata.get("ai_metadata")
                    # ç¡®ä¿ ai_metadata æ°¸è¿œæ˜¯ dictï¼Œnever None
                    # PostgreSQL gRPC may return protobuf Struct, convert to dict
                    if ai_metadata_extracted is not None and not isinstance(
                        ai_metadata_extracted, dict
                    ):
                        # Try to convert protobuf Struct to dict
                        from google.protobuf.json_format import MessageToDict

                        try:
                            ai_metadata_extracted = MessageToDict(
                                ai_metadata_extracted, preserving_proto_field_name=True
                            )
                        except:
                            logger.warning(
                                f"Failed to convert ai_metadata from protobuf, setting to empty dict"
                            )
                            ai_metadata_extracted = {}
                    if not ai_metadata_extracted:
                        ai_metadata_extracted = {}
                    operation_id = doc_metadata.get("operation_id", "unknown")

                    logger.info(
                        f"ğŸ“¤ Preparing file.uploaded.with_ai event for {file_id}"
                    )
                    logger.info(f"  AI metadata extracted: {ai_metadata_extracted}")
                    logger.info(f"  operation_id/chunk_id: {operation_id}")

                    await publisher.publish_file_uploaded_with_ai(
                        file_id=file_id,
                        file_name=file_name,
                        file_size=file_size,
                        content_type=file_type,
                        user_id=user_id,
                        organization_id=organization_id,
                        access_level="private",
                        download_url=file_content
                        if file_content.startswith("http")
                        else "",
                        bucket_name=bucket_name,
                        object_name=object_name,
                        chunk_id=operation_id,
                        ai_metadata=ai_metadata_extracted,
                    )

        except Exception as e:
            logger.error(f"Failed to index file {file_id}: {e}")
            # å‘å¸ƒç´¢å¼•å¤±è´¥äº‹ä»¶
            if event_bus:
                from .publishers import StorageEventPublisher

                publisher = StorageEventPublisher(event_bus)
                await publisher.publish_file_indexing_failed(
                    file_id=file_id, user_id=user_id, error=str(e)
                )

    except Exception as e:
        logger.error(f"Error handling file indexing request: {e}")


# æ·»åŠ å…¶ä»– event handlersï¼ˆå¦‚æœéœ€è¦è®¢é˜…å…¶ä»–æœåŠ¡çš„äº‹ä»¶ï¼‰
# ä¾‹å¦‚ï¼š
# async def handle_album_created(event: Event, storage_service):
#     """å¤„ç†ç›¸å†Œåˆ›å»ºäº‹ä»¶"""
#     pass
